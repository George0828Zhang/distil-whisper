seed: 1899
output_dir: ./whisper_base_ft
model_name_or_path: openai/whisper-base
attn_implementation: sdpa
train_dataset_name: yodas_30s
train_dataset_config_name: default
train_split_name: validation
text_column_name: text
eval_dataset_name: yodas_30s
eval_dataset_config_name: default
eval_text_column_name: text
eval_split_name: validation
timestamp_probability: 1.0
condition_on_prev_probability: 1.0
flipscript_probability: 0.5
return_timestamps: true
language: zh
freeze_encoder: true
freeze_decoder: false
freeze_embed_positions: false
dtype: float16
use_schedulefree: true
save_total_limit: 10
save_lora_limit: 50
eval_steps: 2
save_steps: 2
warmup_steps: 50
per_device_train_batch_size: 64
per_device_eval_batch_size: 32
gradient_accumulation_steps: 1
gradient_checkpointing: true
warmup_steps: 800
max_steps: 10
learning_rate: 0.00005
lr_scheduler_type: constant_with_warmup
weight_decay: 0.01
predict_with_generate: true
generation_max_length: 225
logging_steps: 1
report_to: all
evaluation_strategy: steps
save_strategy: steps
do_train: true
do_eval: true
overwrite_output_dir: true
lora_rank: 32
lora_alpha: 64
lora_target: "[qv]_proj$"